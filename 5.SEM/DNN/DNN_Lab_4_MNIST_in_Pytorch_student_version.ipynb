{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ],
   "metadata": {
    "id": "MxW4dJFDfX_a"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcTwzhX8fBqs"
   },
   "source": [
    "Code based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "In this exercise we are using high level abstractions from torch.nn like nn.Linear.\n",
    "Note: during the next lab session we will go one level deeper and implement more things\n",
    "with bare hands.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "    1. Read the code.\n",
    "\n",
    "    2. Check that the given implementation reaches 95% test accuracy for architecture input-128-128-10 after few epochs.\n",
    "\n",
    "    3. Add the option to use SGD with momentum instead of ADAM.\n",
    "\n",
    "    4. Experiment with different learning rates, plot the learning curves for different\n",
    "    learning rates for both ADAM and SGD with momentum.\n",
    "\n",
    "    5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
    "    Note that this requires creating a list of layers as an atribute of the Net class,\n",
    "    and one can't use a standard python list containing nn.Modules (why?).\n",
    "    Check torch.nn.ModuleList.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IYAsziKffBFV",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:49:54.251699Z",
     "start_time": "2023-11-22T00:49:53.248070Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DMtap4QCfBH8",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:49:54.256648Z",
     "start_time": "2023-11-22T00:49:54.253975Z"
    }
   },
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # After flattening an image of size 28x28 we have 784 inputs\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return correct / len(test_loader.dataset)\n",
    "\n"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K5GlMs1-fBKP",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:49:54.258102Z",
     "start_time": "2023-11-22T00:49:54.256468Z"
    }
   },
   "source": [
    "batch_size = 256\n",
    "test_batch_size = 1000\n",
    "epochs = 5\n",
    "lr = 1e-2\n",
    "seed = 1\n",
    "log_interval = 10\n",
    "use_cuda = torch.cuda.is_available()"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WgfUP23AfBMd",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:49:54.262453Z",
     "start_time": "2023-11-22T00:49:54.258622Z"
    }
   },
   "source": [
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                    'pin_memory': True,\n",
    "                    'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0KPoUtsfBOs",
    "outputId": "4ee308b0-0aac-4d3c-f372-352f28970104",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:49:54.304182Z",
     "start_time": "2023-11-22T00:49:54.262981Z"
    }
   },
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Check that the given implementation reaches 95% test accuracy for architecture input-128-128-10 after few epochs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezvIQbgsfBRT",
    "outputId": "3f6621ef-0bad-46c6-bd8f-ac535db8e9af",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:50:05.733677Z",
     "start_time": "2023-11-22T00:49:54.305140Z"
    }
   },
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "provided_net_accuracy = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "    provided_net_accuracy.append(test(model, device, test_loader))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.313259\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.732760\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.582761\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.481087\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.334523\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.346265\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.229349\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.191099\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.224889\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.244064\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.236049\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.241033\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.194208\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.302404\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.203934\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.192445\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.173173\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.140804\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.356526\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.130328\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.193289\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.220832\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.161527\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.032843\n",
      "\n",
      "Test set: Average loss: 0.2283, Accuracy: 9316/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.192632\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.183039\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.220922\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.215665\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.127777\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.206086\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.094744\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.093734\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.119091\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.136629\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.211988\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.153911\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.129250\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.177700\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.181928\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.130971\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.166735\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.129824\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.229716\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.108692\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.117482\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.167308\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.097701\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.006758\n",
      "\n",
      "Test set: Average loss: 0.1426, Accuracy: 9602/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.135243\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.142902\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.115922\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.153900\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.109228\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.114536\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.083648\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.125375\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.108079\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.086800\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.120213\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.125308\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.064900\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.187379\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.095151\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.123021\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.101951\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.094606\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.189450\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.091905\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.090419\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.097320\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.153882\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.005421\n",
      "\n",
      "Test set: Average loss: 0.1478, Accuracy: 9598/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.099989\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.134014\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.110899\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.098086\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.088740\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.079186\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.055431\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.108219\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.057505\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.079723\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.113280\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.173531\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.103068\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.076178\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.075656\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.071970\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.063176\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.064589\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.118455\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.051017\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.106753\n",
      "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.090745\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.115660\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.004415\n",
      "\n",
      "Test set: Average loss: 0.1548, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.112617\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.100344\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.125359\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.094716\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.118584\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.182294\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.078369\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.098103\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.068625\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.061692\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.095244\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.102119\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.076537\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.083099\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.131516\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.064879\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.065746\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.051042\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.111374\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.018209\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.110297\n",
      "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.074148\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.081731\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.005173\n",
      "\n",
      "Test set: Average loss: 0.1711, Accuracy: 9562/10000 (96%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "  3. Add the option to use SGD with momentum instead of ADAM.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQMSSwuifBTo",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:50:16.818198Z",
     "start_time": "2023-11-22T00:50:05.734326Z"
    }
   },
   "source": [
    "model2 = Net().to(device)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=lr, momentum=0.95)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model2, device, train_loader, optimizer2, epoch, log_interval)\n",
    "    test(model2, device, test_loader)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297579\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.194206\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.830583\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.151425\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.561842\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.649637\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.362383\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.346002\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.512563\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.397074\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.378667\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.503401\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.385932\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.363953\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.320298\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.298104\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.300542\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.255723\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.390671\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.150856\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.215200\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.276470\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.207231\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.042935\n",
      "\n",
      "Test set: Average loss: 0.2054, Accuracy: 9399/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.220172\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.197353\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.267947\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.266808\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.199660\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.222140\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.161010\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.182114\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.236535\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.155933\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.200577\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.197880\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.180289\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.190226\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.202159\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.177597\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.177152\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.143918\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.263868\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.075027\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.127877\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.172380\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.120353\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.019301\n",
      "\n",
      "Test set: Average loss: 0.1324, Accuracy: 9594/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.150333\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.126769\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.155368\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.151424\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.144707\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.127822\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.113026\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.098025\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.154416\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.099933\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.152435\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.150379\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.134904\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.135860\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.132088\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.126357\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.127512\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.106147\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.179873\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.054710\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.109538\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.113769\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.088888\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.012254\n",
      "\n",
      "Test set: Average loss: 0.1066, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.119041\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.097951\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.123279\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.114806\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.101755\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.085835\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.083558\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.064186\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.124704\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.071862\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.115609\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.105049\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.113062\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.113127\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.097499\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.101270\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.103084\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.092229\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.145115\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.046542\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.098008\n",
      "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.078397\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.071239\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.009097\n",
      "\n",
      "Test set: Average loss: 0.0942, Accuracy: 9704/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.096264\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.081403\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.093567\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.085241\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.081459\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070867\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.063655\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.050958\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.101253\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.058722\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.092784\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.080827\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.099004\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.096252\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.074168\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.085590\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.089404\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.086222\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.124606\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.042619\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.081057\n",
      "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.056796\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.059559\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.008223\n",
      "\n",
      "Test set: Average loss: 0.0882, Accuracy: 9729/10000 (97%)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Experiment with different learning rates, plot the learning curves for different\n",
    "    learning rates for both ADAM and SGD with momentum."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.005, 0.01, 0.015, 0.02, 0.025]\n"
     ]
    }
   ],
   "source": [
    "min_lr = 0.005\n",
    "max_lr = 0.025\n",
    "n_lr = 5\n",
    "sgd_momentum = 0.9\n",
    "\n",
    "lr_log_interval = 115\n",
    "\n",
    "learning_rates = [round(x * ((max_lr - min_lr) / (n_lr - 1)) + min_lr, 3) for x in range(n_lr)]\n",
    "\n",
    "lr_accuracy_sgd = {}\n",
    "lr_accuracy_adam = {}\n",
    "\n",
    "print(learning_rates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:50:16.822914Z",
     "start_time": "2023-11-22T00:50:16.819019Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JX_2rCycfBWU",
    "ExecuteTime": {
     "end_time": "2023-11-22T00:52:11.696985Z",
     "start_time": "2023-11-22T00:50:16.820980Z"
    }
   },
   "source": [
    "for learning_rate in learning_rates:\n",
    "    print(learning_rate)\n",
    "    model_sgd = Net().to(device)\n",
    "    sgd_optimizer = optim.SGD(model_sgd.parameters(), lr=learning_rate, momentum=sgd_momentum)\n",
    "    sgd_accuracy = []\n",
    "    \n",
    "    model_adam = Net().to(device)\n",
    "    adam_optimizer = optim.Adam(model_adam.parameters(), lr=learning_rate)\n",
    "    adam_accuracy = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model_adam, device, train_loader, adam_optimizer, epoch, lr_log_interval)\n",
    "        adam_accuracy.append(test(model_adam, device, test_loader))\n",
    "        \n",
    "        train(model_sgd, device, train_loader, sgd_optimizer, epoch, lr_log_interval)\n",
    "        sgd_accuracy.append(test(model_sgd, device, test_loader))\n",
    "        \n",
    "    lr_accuracy_sgd[learning_rate] = sgd_accuracy\n",
    "    lr_accuracy_adam[learning_rate] = adam_accuracy\n",
    "\n",
    "\n",
    "print(\"sgd: \", lr_accuracy_sgd)\n",
    "print(\"adam: \", lr_accuracy_adam)\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.292853\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.188669\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.036438\n",
      "\n",
      "Test set: Average loss: 0.2057, Accuracy: 9313/10000 (93%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.299345\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.437326\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.094941\n",
      "\n",
      "Test set: Average loss: 0.3243, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.217585\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.067767\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.004512\n",
      "\n",
      "Test set: Average loss: 0.1286, Accuracy: 9616/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.319675\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.243473\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.057007\n",
      "\n",
      "Test set: Average loss: 0.2547, Accuracy: 9266/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.137910\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.067167\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.007931\n",
      "\n",
      "Test set: Average loss: 0.1402, Accuracy: 9574/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.248116\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.183118\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.044093\n",
      "\n",
      "Test set: Average loss: 0.2148, Accuracy: 9364/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.132992\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.042926\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.002679\n",
      "\n",
      "Test set: Average loss: 0.1368, Accuracy: 9609/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.213626\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.143643\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.035705\n",
      "\n",
      "Test set: Average loss: 0.1861, Accuracy: 9441/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.097576\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.034583\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.006990\n",
      "\n",
      "Test set: Average loss: 0.1447, Accuracy: 9587/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.191129\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.116482\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.030366\n",
      "\n",
      "Test set: Average loss: 0.1646, Accuracy: 9514/10000 (95%)\n",
      "\n",
      "0.01\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.296072\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.148897\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.018277\n",
      "\n",
      "Test set: Average loss: 0.2047, Accuracy: 9375/10000 (94%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308394\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.350883\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.075088\n",
      "\n",
      "Test set: Average loss: 0.2679, Accuracy: 9216/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.195654\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.084301\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.007950\n",
      "\n",
      "Test set: Average loss: 0.1615, Accuracy: 9524/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.274666\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.161889\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.038286\n",
      "\n",
      "Test set: Average loss: 0.1859, Accuracy: 9436/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.122967\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.079443\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.014572\n",
      "\n",
      "Test set: Average loss: 0.1723, Accuracy: 9523/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.196312\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.112224\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.025190\n",
      "\n",
      "Test set: Average loss: 0.1466, Accuracy: 9544/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.170623\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.109541\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.020947\n",
      "\n",
      "Test set: Average loss: 0.1889, Accuracy: 9528/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.161292\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.087496\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.017190\n",
      "\n",
      "Test set: Average loss: 0.1240, Accuracy: 9615/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.131653\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.130206\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.005567\n",
      "\n",
      "Test set: Average loss: 0.1533, Accuracy: 9645/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.137306\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.070330\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.011861\n",
      "\n",
      "Test set: Average loss: 0.1099, Accuracy: 9662/10000 (97%)\n",
      "\n",
      "0.015\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306113\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.169317\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.069450\n",
      "\n",
      "Test set: Average loss: 0.2596, Accuracy: 9300/10000 (93%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300774\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.317671\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.058930\n",
      "\n",
      "Test set: Average loss: 0.2361, Accuracy: 9273/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.242097\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.148485\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.038595\n",
      "\n",
      "Test set: Average loss: 0.1808, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.242373\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.139323\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.028687\n",
      "\n",
      "Test set: Average loss: 0.1518, Accuracy: 9533/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.160048\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.100863\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.036984\n",
      "\n",
      "Test set: Average loss: 0.2144, Accuracy: 9401/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.156627\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.086453\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.017565\n",
      "\n",
      "Test set: Average loss: 0.1210, Accuracy: 9631/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.296419\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.086067\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.016258\n",
      "\n",
      "Test set: Average loss: 0.1949, Accuracy: 9498/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.116813\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.063069\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.014130\n",
      "\n",
      "Test set: Average loss: 0.1050, Accuracy: 9672/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.155839\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.129141\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.031216\n",
      "\n",
      "Test set: Average loss: 0.1956, Accuracy: 9516/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.095385\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.050598\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.012092\n",
      "\n",
      "Test set: Average loss: 0.0963, Accuracy: 9696/10000 (97%)\n",
      "\n",
      "0.02\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308634\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.197802\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.020434\n",
      "\n",
      "Test set: Average loss: 0.2856, Accuracy: 9166/10000 (92%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.325960\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.264127\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.047281\n",
      "\n",
      "Test set: Average loss: 0.2079, Accuracy: 9357/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.289188\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.136500\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.058955\n",
      "\n",
      "Test set: Average loss: 0.2377, Accuracy: 9382/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.223829\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.110265\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.023080\n",
      "\n",
      "Test set: Average loss: 0.1359, Accuracy: 9578/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.245362\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.097805\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.035271\n",
      "\n",
      "Test set: Average loss: 0.2261, Accuracy: 9410/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.141410\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.069819\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.014454\n",
      "\n",
      "Test set: Average loss: 0.1094, Accuracy: 9649/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.159725\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.099089\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.049158\n",
      "\n",
      "Test set: Average loss: 0.2356, Accuracy: 9464/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.108669\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.050824\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.011703\n",
      "\n",
      "Test set: Average loss: 0.0976, Accuracy: 9683/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.156511\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.127448\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.045772\n",
      "\n",
      "Test set: Average loss: 0.2414, Accuracy: 9407/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.092552\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.043281\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.009003\n",
      "\n",
      "Test set: Average loss: 0.0918, Accuracy: 9705/10000 (97%)\n",
      "\n",
      "0.025\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301022\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.264107\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.063584\n",
      "\n",
      "Test set: Average loss: 0.3062, Accuracy: 9132/10000 (91%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312456\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.232675\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.037142\n",
      "\n",
      "Test set: Average loss: 0.2012, Accuracy: 9377/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.309412\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.208784\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.017640\n",
      "\n",
      "Test set: Average loss: 0.2736, Accuracy: 9281/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.205882\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.091007\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.018631\n",
      "\n",
      "Test set: Average loss: 0.1197, Accuracy: 9624/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.222374\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.199056\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.026563\n",
      "\n",
      "Test set: Average loss: 0.2594, Accuracy: 9373/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.121610\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.061996\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.011520\n",
      "\n",
      "Test set: Average loss: 0.1021, Accuracy: 9672/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.229079\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.208461\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.028430\n",
      "\n",
      "Test set: Average loss: 0.2550, Accuracy: 9410/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.102757\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.044949\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.008472\n",
      "\n",
      "Test set: Average loss: 0.0918, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.232060\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.241261\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.026217\n",
      "\n",
      "Test set: Average loss: 0.2591, Accuracy: 9359/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.090513\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.031488\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.006685\n",
      "\n",
      "Test set: Average loss: 0.0877, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "sgd:  {0.005: [0.9063, 0.9266, 0.9364, 0.9441, 0.9514], 0.01: [0.9216, 0.9436, 0.9544, 0.9615, 0.9662], 0.015: [0.9273, 0.9533, 0.9631, 0.9672, 0.9696], 0.02: [0.9357, 0.9578, 0.9649, 0.9683, 0.9705], 0.025: [0.9377, 0.9624, 0.9672, 0.9715, 0.9724]}\n",
      "adam:  {0.005: [0.9313, 0.9616, 0.9574, 0.9609, 0.9587], 0.01: [0.9375, 0.9524, 0.9523, 0.9528, 0.9645], 0.015: [0.93, 0.9499, 0.9401, 0.9498, 0.9516], 0.02: [0.9166, 0.9382, 0.941, 0.9464, 0.9407], 0.025: [0.9132, 0.9281, 0.9373, 0.941, 0.9359]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=dd6450c2b9c641f292dc38abc040deec\n",
      "2023-11-22 01:52:17,136 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/6d5d3af9d7ed409da0bb697700672a75/experiments/dd6450c2b9c641f292dc38abc040deec/output/log\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task, Logger\n",
    "task = Task.init(project_name=\"DNN\", task_name=\"Lab 3 - pytorch\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:52:18.528510Z",
     "start_time": "2023-11-22T00:52:11.698151Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_plots(task=task, sgd_accuracy=lr_accuracy_sgd, adam_accuracy=lr_accuracy_adam):\n",
    "    logger = task.get_logger()\n",
    "    for learn_rate, accuracy in sgd_accuracy.items():\n",
    "        sgd_hstack = np.hstack((np.atleast_2d(np.arange(1, len(accuracy) + 1)).T, np.atleast_2d(np.array(accuracy)).T))\n",
    "        logger.report_scatter2d(\n",
    "            \"learn rate comparison\",\n",
    "            \"sgd \" + str(learn_rate),\n",
    "            scatter=sgd_hstack,\n",
    "            xaxis=\"epoch\",\n",
    "            yaxis=\"accuracy\"\n",
    "        )\n",
    "    for learn_rate, accuracy in adam_accuracy.items():\n",
    "        adam_hstack = np.hstack((np.atleast_2d(np.arange(1, len(accuracy) + 1)).T, np.atleast_2d(np.array(accuracy)).T))\n",
    "        logger.report_scatter2d(\n",
    "            \"learn rate comparison\",\n",
    "            \"adam \" + str(learn_rate),\n",
    "            scatter=adam_hstack,\n",
    "            xaxis=\"epoch\",\n",
    "            yaxis=\"accuracy\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "create_plots()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:52:18.627442Z",
     "start_time": "2023-11-22T00:52:18.531744Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<!doctype html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\">\n    <style> * {\n        margin: 0;\n        padding: 0;\n        box-sizing: border-box;\n    }\n\n    html, body {\n        height: 100%;\n        overflow: hidden;\n    }\n\n    body {\n        font-family: sans-serif;\n        font-size: 14px;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        color: #384161;\n        padding: 12px;\n    }\n\n    .content {\n        margin: 4px;\n        border-radius: 4px;\n        width: 100%;\n        height: 100%;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        flex-direction: column;\n        border: 1px solid #dce0ee;\n    }\n\n    .cml-logo {\n        margin-bottom: 12px;\n    }\n\n    .login-link {\n        color: #4d66ff;\n        text-decoration: none;\n    }\n\n    .login-link:hover {\n        text-decoration: underline;\n    } </style>\n<body>\n<iframe   src=\"https://app.clear.ml/widgets/?type=plot&objectType=task&objects=dd6450c2b9c641f292dc38abc040deec&metrics=learn%20rate%20comparison&variants=adam%200.005&variants=adam%200.01&variants=adam%200.015&variants=adam%200.02&variants=adam%200.025&variants=sgd%200.005&variants=sgd%200.01&variants=sgd%200.015&variants=sgd%200.02&variants=sgd%200.025&company=7964db38bc90424c804b4834f24f7440\"   name=\"c767aab3-fdf4-4524-999c-767582ec7bf5\"   width=\"1000\" height=\"400\" ></iframe>\n</body>\n</html>\n"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(filename=\"plot.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T01:20:59.101167Z",
     "start_time": "2023-11-22T01:20:59.095503Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Parameterize the constructor by a list of sizes of hidden layers of the MLP.\n",
    "    Note that this requires creating a list of layers as an atribute of the Net class,\n",
    "    and one can't use a standard python list containing nn.Modules (why?).\n",
    "    Check torch.nn.ModuleList."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can't use python list of nn.Modules because model.parameters() won't work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class NetParameterized(nn.Module):\n",
    "    def __init__(self, hidden_layers):\n",
    "        super(NetParameterized, self).__init__()\n",
    "        # After flattening an image of size 28x28 we have 784 inputs\n",
    "        last_layer = 784\n",
    "        linears = []\n",
    "        for current_layer in hidden_layers:\n",
    "            print(f\"Created layer: ({last_layer}, {current_layer})\")\n",
    "            linears.append(nn.Linear(last_layer, current_layer))\n",
    "            last_layer = current_layer\n",
    "        self.linears = nn.ModuleList(linears)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        for layer in self.linears[:-1]:\n",
    "            x = layer(x)\n",
    "            x = F.relu(x)\n",
    "        x = self.linears[-1](x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:52:18.669564Z",
     "start_time": "2023-11-22T00:52:18.638986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created layer: (784, 512)\n",
      "Created layer: (512, 256)\n",
      "Created layer: (256, 128)\n",
      "Created layer: (128, 96)\n",
      "Created layer: (96, 64)\n",
      "Created layer: (64, 32)\n",
      "Created layer: (32, 10)\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314109\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.735290\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.254255\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.086724\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.977643\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.986996\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.654195\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.493050\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.653174\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.511010\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.461986\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.383572\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.503253\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.369749\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.302052\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.297550\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.273930\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.233112\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.328857\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.207936\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.307615\n",
      "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.302457\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.179704\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.013666\n",
      "\n",
      "Test set: Average loss: 0.2585, Accuracy: 9397/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.271603\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.236483\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.260147\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.247357\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.216510\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.290562\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.171775\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.130686\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.265246\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.143001\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.207824\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.204859\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.209600\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.191533\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.194930\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.168070\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.289951\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.159801\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.288259\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.129411\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.177531\n",
      "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.197701\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.125805\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.014012\n",
      "\n",
      "Test set: Average loss: 0.2036, Accuracy: 9537/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.196345\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.169972\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.262278\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.188072\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.129239\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.227967\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.128680\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.091597\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.215882\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.121802\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.205866\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.226737\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.152611\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.119288\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.137065\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.126790\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.155735\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.092939\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.198873\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.134137\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.160864\n",
      "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.149388\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.104700\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.020360\n",
      "\n",
      "Test set: Average loss: 0.1755, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.165416\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.144938\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.157232\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.132639\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.086731\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.193905\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.112053\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.103691\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.180270\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.072666\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.156614\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.165949\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.077186\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.121038\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.168862\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.111122\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.179379\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.097387\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.158360\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.090069\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.126102\n",
      "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.162945\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.105189\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.006685\n",
      "\n",
      "Test set: Average loss: 0.1577, Accuracy: 9640/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.108765\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.111272\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.137390\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.184139\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.079638\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.104391\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.128381\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.086063\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.121463\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.085796\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.157474\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.149943\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.146658\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.160193\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.117388\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.079107\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.097057\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.077565\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.238208\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.063190\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.080751\n",
      "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.132550\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.092419\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.062025\n",
      "\n",
      "Test set: Average loss: 0.2160, Accuracy: 9640/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "modelParameterized = NetParameterized([512, 256, 128, 96, 64, 32, 10]).to(device)\n",
    "optimizer = optim.Adam(modelParameterized.parameters(), lr=lr)\n",
    "\n",
    "parameterized_net_accuracy = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(modelParameterized, device, train_loader, optimizer, epoch, log_interval)\n",
    "    parameterized_net_accuracy.append(test(modelParameterized, device, test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:55:10.132063Z",
     "start_time": "2023-11-22T00:54:55.251906Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(provided_net_accuracy, 'r')\n",
    "plt.plot(parameterized_net_accuracy, 'b')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:55:14.443997Z",
     "start_time": "2023-11-22T00:55:13.685409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-22T00:52:34.150960Z",
     "start_time": "2023-11-22T00:52:34.149264Z"
    }
   }
  }
 ]
}
