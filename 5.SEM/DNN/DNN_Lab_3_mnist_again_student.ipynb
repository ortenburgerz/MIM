{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84VetyCaGLyR"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "P22HqX9AbO1a"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9jGPaZhbO2B",
    "outputId": "d2d5a21e-8666-48e2-c837-4a64a87d1127"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 94054608.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 66194368.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 23977545.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 15875440.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's read the mnist dataset\n",
    "\n",
    "def load_mnist(path='.'):\n",
    "    train_set = datasets.MNIST(path, train=True, download=True)\n",
    "    x_train = train_set.data.numpy()\n",
    "    _y_train = train_set.targets.numpy()\n",
    "\n",
    "    test_set = datasets.MNIST(path, train=False, download=True)\n",
    "    x_test = test_set.data.numpy()\n",
    "    _y_test = test_set.targets.numpy()\n",
    "\n",
    "    x_train = x_train.reshape((x_train.shape[0],28*28)) / 255.\n",
    "    x_test = x_test.reshape((x_test.shape[0],28*28)) / 255.\n",
    "\n",
    "    y_train = np.zeros((_y_train.shape[0], 10))\n",
    "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
    "\n",
    "    y_test = np.zeros((_y_test.shape[0], 10))\n",
    "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziZ9i7tXbO1T"
   },
   "source": [
    "In this lab, you will implement some of the techniques discussed in the lecture.\n",
    "\n",
    "Below you are given a solution to the previous scenario. Note that it has two serious drawbacks:\n",
    " * The output predictions do not sum up to one (i.e. it does not return a distribution) even though the images always contain exactly one digit.\n",
    " * It uses MSE coupled with output sigmoid which can lead to saturation and slow convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgEA2XRRbO2X",
    "outputId": "bf3de370-e88e-4e02-f595-8a43f82480d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.8241\n",
      "Epoch: 1, Accuracy: 0.8749\n",
      "Epoch: 2, Accuracy: 0.8912\n",
      "Epoch: 3, Accuracy: 0.9006\n",
      "Epoch: 4, Accuracy: 0.9074\n",
      "Epoch: 5, Accuracy: 0.9125\n",
      "Epoch: 6, Accuracy: 0.914\n",
      "Epoch: 7, Accuracy: 0.9173\n",
      "Epoch: 8, Accuracy: 0.9192\n",
      "Epoch: 9, Accuracy: 0.9204\n",
      "Epoch: 10, Accuracy: 0.9236\n",
      "Epoch: 11, Accuracy: 0.9244\n",
      "Epoch: 12, Accuracy: 0.9264\n",
      "Epoch: 13, Accuracy: 0.9279\n",
      "Epoch: 14, Accuracy: 0.9298\n",
      "Epoch: 15, Accuracy: 0.9305\n",
      "Epoch: 16, Accuracy: 0.9313\n",
      "Epoch: 17, Accuracy: 0.932\n",
      "Epoch: 18, Accuracy: 0.9322\n",
      "Epoch: 19, Accuracy: 0.9329\n",
      "Epoch: 20, Accuracy: 0.9343\n",
      "Epoch: 21, Accuracy: 0.9353\n",
      "Epoch: 22, Accuracy: 0.9358\n",
      "Epoch: 23, Accuracy: 0.9364\n",
      "Epoch: 24, Accuracy: 0.9372\n",
      "Epoch: 25, Accuracy: 0.9375\n",
      "Epoch: 26, Accuracy: 0.9379\n",
      "Epoch: 27, Accuracy: 0.9385\n",
      "Epoch: 28, Accuracy: 0.939\n",
      "Epoch: 29, Accuracy: 0.939\n",
      "Epoch: 30, Accuracy: 0.9391\n",
      "Epoch: 31, Accuracy: 0.9394\n",
      "Epoch: 32, Accuracy: 0.9397\n",
      "Epoch: 33, Accuracy: 0.9397\n",
      "Epoch: 34, Accuracy: 0.9399\n",
      "Epoch: 35, Accuracy: 0.9405\n",
      "Epoch: 36, Accuracy: 0.941\n",
      "Epoch: 37, Accuracy: 0.9415\n",
      "Epoch: 38, Accuracy: 0.9419\n",
      "Epoch: 39, Accuracy: 0.9417\n",
      "Epoch: 40, Accuracy: 0.9421\n",
      "Epoch: 41, Accuracy: 0.9426\n",
      "Epoch: 42, Accuracy: 0.9433\n",
      "Epoch: 43, Accuracy: 0.9432\n",
      "Epoch: 44, Accuracy: 0.9434\n",
      "Epoch: 45, Accuracy: 0.9436\n",
      "Epoch: 46, Accuracy: 0.9436\n",
      "Epoch: 47, Accuracy: 0.9437\n",
      "Epoch: 48, Accuracy: 0.9434\n",
      "Epoch: 49, Accuracy: 0.9432\n",
      "Epoch: 50, Accuracy: 0.9432\n",
      "Epoch: 51, Accuracy: 0.9436\n",
      "Epoch: 52, Accuracy: 0.9437\n",
      "Epoch: 53, Accuracy: 0.9437\n",
      "Epoch: 54, Accuracy: 0.9438\n",
      "Epoch: 55, Accuracy: 0.9435\n",
      "Epoch: 56, Accuracy: 0.9435\n",
      "Epoch: 57, Accuracy: 0.9437\n",
      "Epoch: 58, Accuracy: 0.9438\n",
      "Epoch: 59, Accuracy: 0.9441\n",
      "Epoch: 60, Accuracy: 0.944\n",
      "Epoch: 61, Accuracy: 0.944\n",
      "Epoch: 62, Accuracy: 0.9441\n",
      "Epoch: 63, Accuracy: 0.9444\n",
      "Epoch: 64, Accuracy: 0.9445\n",
      "Epoch: 65, Accuracy: 0.9447\n",
      "Epoch: 66, Accuracy: 0.9448\n",
      "Epoch: 67, Accuracy: 0.945\n",
      "Epoch: 68, Accuracy: 0.945\n",
      "Epoch: 69, Accuracy: 0.9448\n",
      "Epoch: 70, Accuracy: 0.9448\n",
      "Epoch: 71, Accuracy: 0.9446\n",
      "Epoch: 72, Accuracy: 0.945\n",
      "Epoch: 73, Accuracy: 0.9448\n",
      "Epoch: 74, Accuracy: 0.9451\n",
      "Epoch: 75, Accuracy: 0.9453\n",
      "Epoch: 76, Accuracy: 0.9455\n",
      "Epoch: 77, Accuracy: 0.9453\n",
      "Epoch: 78, Accuracy: 0.9452\n",
      "Epoch: 79, Accuracy: 0.9452\n",
      "Epoch: 80, Accuracy: 0.9453\n",
      "Epoch: 81, Accuracy: 0.9454\n",
      "Epoch: 82, Accuracy: 0.9458\n",
      "Epoch: 83, Accuracy: 0.9462\n",
      "Epoch: 84, Accuracy: 0.9463\n",
      "Epoch: 85, Accuracy: 0.9464\n",
      "Epoch: 86, Accuracy: 0.9465\n",
      "Epoch: 87, Accuracy: 0.9465\n",
      "Epoch: 88, Accuracy: 0.9467\n",
      "Epoch: 89, Accuracy: 0.9467\n",
      "Epoch: 90, Accuracy: 0.9469\n",
      "Epoch: 91, Accuracy: 0.947\n",
      "Epoch: 92, Accuracy: 0.947\n",
      "Epoch: 93, Accuracy: 0.9471\n",
      "Epoch: 94, Accuracy: 0.9469\n",
      "Epoch: 95, Accuracy: 0.9469\n",
      "Epoch: 96, Accuracy: 0.9471\n",
      "Epoch: 97, Accuracy: 0.9471\n",
      "Epoch: 98, Accuracy: 0.9471\n",
      "Epoch: 99, Accuracy: 0.9471\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        # initialize biases and weights with random normal distr.\n",
    "        # weights are indexed by target node first\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    def feedforward(self, a):\n",
    "        # Run the network on a batch\n",
    "        a = a.T\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.matmul(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # Update networks weights and biases by applying a single step\n",
    "        # of gradient descent using backpropagation to compute the gradient.\n",
    "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
    "        # eta is the learning rate\n",
    "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
    "\n",
    "        self.weights = [w-(eta/len(mini_batch[0]))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch[0]))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a pair of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "        g = x\n",
    "        gs = [g] # list to store all the gs, layer by layer\n",
    "        fs = [] # list to store all the fs, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            f = np.dot(w, g)+b\n",
    "            fs.append(f)\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "        # backward pass <- both steps at once\n",
    "        dLdg = self.cost_derivative(gs[-1], y)\n",
    "        dLdfs = []\n",
    "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
    "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
    "            dLdfs.append(dLdf)\n",
    "            dLdg = np.matmul(w.T, dLdf)\n",
    "\n",
    "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])]\n",
    "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)]\n",
    "        return (dLdBs,dLdWs)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
    "        corr = np.argmax(test_data[1],axis=1).T\n",
    "        return np.mean(pred==corr)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        x_train, y_train = training_data\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "        for j in range(epochs):\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
    "            else:\n",
    "                print(\"Epoch: {0}\".format(j))\n",
    "\n",
    "\n",
    "network = Network([784,30,10])\n",
    "network.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNIHlO-qlb1S"
   },
   "source": [
    "\n",
    "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two function as a single block and not even compute the gradient over the softmax values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4_os2m7meLk",
    "outputId": "3ae50cae-b50a-4591-e6ec-047fe26d02c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.7909\n",
      "Epoch: 1, Accuracy: 0.8536\n",
      "Epoch: 2, Accuracy: 0.8786\n",
      "Epoch: 3, Accuracy: 0.8915\n",
      "Epoch: 4, Accuracy: 0.8981\n",
      "Epoch: 5, Accuracy: 0.9041\n",
      "Epoch: 6, Accuracy: 0.9082\n",
      "Epoch: 7, Accuracy: 0.9107\n",
      "Epoch: 8, Accuracy: 0.9129\n",
      "Epoch: 9, Accuracy: 0.9138\n",
      "Epoch: 10, Accuracy: 0.9145\n",
      "Epoch: 11, Accuracy: 0.915\n",
      "Epoch: 12, Accuracy: 0.9166\n",
      "Epoch: 13, Accuracy: 0.9181\n",
      "Epoch: 14, Accuracy: 0.9205\n",
      "Epoch: 15, Accuracy: 0.9217\n",
      "Epoch: 16, Accuracy: 0.9225\n",
      "Epoch: 17, Accuracy: 0.9238\n",
      "Epoch: 18, Accuracy: 0.9248\n",
      "Epoch: 19, Accuracy: 0.9253\n",
      "Epoch: 20, Accuracy: 0.926\n",
      "Epoch: 21, Accuracy: 0.9269\n",
      "Epoch: 22, Accuracy: 0.9274\n",
      "Epoch: 23, Accuracy: 0.9276\n",
      "Epoch: 24, Accuracy: 0.9281\n",
      "Epoch: 25, Accuracy: 0.9293\n",
      "Epoch: 26, Accuracy: 0.929\n",
      "Epoch: 27, Accuracy: 0.9289\n",
      "Epoch: 28, Accuracy: 0.929\n",
      "Epoch: 29, Accuracy: 0.9297\n",
      "Epoch: 30, Accuracy: 0.9293\n",
      "Epoch: 31, Accuracy: 0.9296\n",
      "Epoch: 32, Accuracy: 0.9294\n",
      "Epoch: 33, Accuracy: 0.93\n",
      "Epoch: 34, Accuracy: 0.9306\n",
      "Epoch: 35, Accuracy: 0.931\n",
      "Epoch: 36, Accuracy: 0.9319\n",
      "Epoch: 37, Accuracy: 0.9328\n",
      "Epoch: 38, Accuracy: 0.9328\n",
      "Epoch: 39, Accuracy: 0.9332\n",
      "Epoch: 40, Accuracy: 0.9331\n",
      "Epoch: 41, Accuracy: 0.9338\n",
      "Epoch: 42, Accuracy: 0.9339\n",
      "Epoch: 43, Accuracy: 0.9343\n",
      "Epoch: 44, Accuracy: 0.9343\n",
      "Epoch: 45, Accuracy: 0.9336\n",
      "Epoch: 46, Accuracy: 0.933\n",
      "Epoch: 47, Accuracy: 0.9337\n",
      "Epoch: 48, Accuracy: 0.9339\n",
      "Epoch: 49, Accuracy: 0.934\n",
      "Epoch: 50, Accuracy: 0.934\n",
      "Epoch: 51, Accuracy: 0.934\n",
      "Epoch: 52, Accuracy: 0.9344\n",
      "Epoch: 53, Accuracy: 0.9353\n",
      "Epoch: 54, Accuracy: 0.9356\n",
      "Epoch: 55, Accuracy: 0.9356\n",
      "Epoch: 56, Accuracy: 0.9363\n",
      "Epoch: 57, Accuracy: 0.9357\n",
      "Epoch: 58, Accuracy: 0.9352\n",
      "Epoch: 59, Accuracy: 0.9356\n",
      "Epoch: 60, Accuracy: 0.9365\n",
      "Epoch: 61, Accuracy: 0.9364\n",
      "Epoch: 62, Accuracy: 0.9364\n",
      "Epoch: 63, Accuracy: 0.9369\n",
      "Epoch: 64, Accuracy: 0.9368\n",
      "Epoch: 65, Accuracy: 0.9368\n",
      "Epoch: 66, Accuracy: 0.9362\n",
      "Epoch: 67, Accuracy: 0.9362\n",
      "Epoch: 68, Accuracy: 0.9371\n",
      "Epoch: 69, Accuracy: 0.9371\n",
      "Epoch: 70, Accuracy: 0.9369\n",
      "Epoch: 71, Accuracy: 0.9367\n",
      "Epoch: 72, Accuracy: 0.9369\n",
      "Epoch: 73, Accuracy: 0.9371\n",
      "Epoch: 74, Accuracy: 0.937\n",
      "Epoch: 75, Accuracy: 0.9372\n",
      "Epoch: 76, Accuracy: 0.9371\n",
      "Epoch: 77, Accuracy: 0.9365\n",
      "Epoch: 78, Accuracy: 0.9369\n",
      "Epoch: 79, Accuracy: 0.9374\n",
      "Epoch: 80, Accuracy: 0.9375\n",
      "Epoch: 81, Accuracy: 0.9374\n",
      "Epoch: 82, Accuracy: 0.9374\n",
      "Epoch: 83, Accuracy: 0.9367\n",
      "Epoch: 84, Accuracy: 0.9367\n",
      "Epoch: 85, Accuracy: 0.9363\n",
      "Epoch: 86, Accuracy: 0.9367\n",
      "Epoch: 87, Accuracy: 0.9364\n",
      "Epoch: 88, Accuracy: 0.936\n",
      "Epoch: 89, Accuracy: 0.9366\n",
      "Epoch: 90, Accuracy: 0.9359\n",
      "Epoch: 91, Accuracy: 0.9361\n",
      "Epoch: 92, Accuracy: 0.9361\n",
      "Epoch: 93, Accuracy: 0.9369\n",
      "Epoch: 94, Accuracy: 0.9364\n",
      "Epoch: 95, Accuracy: 0.9357\n",
      "Epoch: 96, Accuracy: 0.9357\n",
      "Epoch: 97, Accuracy: 0.9353\n",
      "Epoch: 98, Accuracy: 0.9349\n",
      "Epoch: 99, Accuracy: 0.9349\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / sum(e_z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "class Network1(object):\n",
    "    def __init__(self, sizes):\n",
    "        # initialize biases and weights with random normal distr.\n",
    "        # weights are indexed by target node first\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    def feedforward(self, a):\n",
    "        a=a.T\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        a = softmax(np.dot(self.weights[-1], a) + self.biases[-1])\n",
    "        return a\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        # Update networks weights and biases by applying a single step\n",
    "        # of gradient descent using backpropagation to compute the gradient.\n",
    "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
    "        # eta is the learning rate\n",
    "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
    "\n",
    "        self.weights = [w-(eta/len(mini_batch[0]))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch[0]))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a pair of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "        g = x\n",
    "        gs = [g] # list to store all the gs, layer by layer\n",
    "        fs = [] # list to store all the fs, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            f = np.dot(w, g) + b\n",
    "            fs.append(f)\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "\n",
    "        # Last layer using softmax\n",
    "        f = np.dot(self.weights[-1], g) + self.biases[-1]\n",
    "        fs.append(f)\n",
    "        g = softmax(f)\n",
    "        gs.append(g)\n",
    "        dLdg = self.cost_derivative(gs[-1], y)\n",
    "        dLdfs = []\n",
    "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
    "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
    "            dLdfs.append(dLdf)\n",
    "            dLdg = np.matmul(w.T, dLdf)\n",
    "\n",
    "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])]\n",
    "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)]\n",
    "        return (dLdBs,dLdWs)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
    "        corr = np.argmax(test_data[1],axis=1).T\n",
    "        return np.mean(pred==corr)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        x_train, y_train = training_data\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "        for j in range(epochs):\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
    "            else:\n",
    "                print(\"Epoch: {0}\".format(j))\n",
    "\n",
    "\n",
    "network1 = Network1([784,30,10])\n",
    "network1.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dy1TJz2llduD"
   },
   "source": [
    "\n",
    "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-ENNNvMrcHP",
    "outputId": "9dccd720-d2be-46a1-9d99-8265db3131ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Accuracy: 0.8485\n",
      "Epoch: 1, Accuracy: 0.8871\n",
      "Epoch: 2, Accuracy: 0.9022\n",
      "Epoch: 3, Accuracy: 0.9075\n",
      "Epoch: 4, Accuracy: 0.9138\n",
      "Epoch: 5, Accuracy: 0.9198\n",
      "Epoch: 6, Accuracy: 0.9227\n",
      "Epoch: 7, Accuracy: 0.9258\n",
      "Epoch: 8, Accuracy: 0.9272\n",
      "Epoch: 9, Accuracy: 0.9269\n",
      "Epoch: 10, Accuracy: 0.9278\n",
      "Epoch: 11, Accuracy: 0.9293\n",
      "Epoch: 12, Accuracy: 0.9291\n",
      "Epoch: 13, Accuracy: 0.9301\n",
      "Epoch: 14, Accuracy: 0.931\n",
      "Epoch: 15, Accuracy: 0.9318\n",
      "Epoch: 16, Accuracy: 0.9317\n",
      "Epoch: 17, Accuracy: 0.932\n",
      "Epoch: 18, Accuracy: 0.9327\n",
      "Epoch: 19, Accuracy: 0.9332\n",
      "Epoch: 20, Accuracy: 0.9331\n",
      "Epoch: 21, Accuracy: 0.9336\n",
      "Epoch: 22, Accuracy: 0.9343\n",
      "Epoch: 23, Accuracy: 0.9351\n",
      "Epoch: 24, Accuracy: 0.9364\n",
      "Epoch: 25, Accuracy: 0.9366\n",
      "Epoch: 26, Accuracy: 0.9369\n",
      "Epoch: 27, Accuracy: 0.937\n",
      "Epoch: 28, Accuracy: 0.9372\n",
      "Epoch: 29, Accuracy: 0.9367\n",
      "Epoch: 30, Accuracy: 0.9359\n",
      "Epoch: 31, Accuracy: 0.9369\n",
      "Epoch: 32, Accuracy: 0.9373\n",
      "Epoch: 33, Accuracy: 0.9379\n",
      "Epoch: 34, Accuracy: 0.9381\n",
      "Epoch: 35, Accuracy: 0.938\n",
      "Epoch: 36, Accuracy: 0.9374\n",
      "Epoch: 37, Accuracy: 0.9372\n",
      "Epoch: 38, Accuracy: 0.937\n",
      "Epoch: 39, Accuracy: 0.9359\n",
      "Epoch: 40, Accuracy: 0.9353\n",
      "Epoch: 41, Accuracy: 0.9362\n",
      "Epoch: 42, Accuracy: 0.9366\n",
      "Epoch: 43, Accuracy: 0.9363\n",
      "Epoch: 44, Accuracy: 0.9379\n",
      "Epoch: 45, Accuracy: 0.9393\n",
      "Epoch: 46, Accuracy: 0.9402\n",
      "Epoch: 47, Accuracy: 0.9403\n",
      "Epoch: 48, Accuracy: 0.9398\n",
      "Epoch: 49, Accuracy: 0.9407\n",
      "Epoch: 50, Accuracy: 0.9406\n",
      "Epoch: 51, Accuracy: 0.9402\n",
      "Epoch: 52, Accuracy: 0.9403\n",
      "Epoch: 53, Accuracy: 0.9411\n",
      "Epoch: 54, Accuracy: 0.9406\n",
      "Epoch: 55, Accuracy: 0.9402\n",
      "Epoch: 56, Accuracy: 0.9401\n",
      "Epoch: 57, Accuracy: 0.9399\n",
      "Epoch: 58, Accuracy: 0.9402\n",
      "Epoch: 59, Accuracy: 0.9409\n",
      "Epoch: 60, Accuracy: 0.9405\n",
      "Epoch: 61, Accuracy: 0.9403\n",
      "Epoch: 62, Accuracy: 0.9405\n",
      "Epoch: 63, Accuracy: 0.9409\n",
      "Epoch: 64, Accuracy: 0.9403\n",
      "Epoch: 65, Accuracy: 0.9396\n",
      "Epoch: 66, Accuracy: 0.9396\n",
      "Epoch: 67, Accuracy: 0.9404\n",
      "Epoch: 68, Accuracy: 0.9407\n",
      "Epoch: 69, Accuracy: 0.9394\n",
      "Epoch: 70, Accuracy: 0.9392\n",
      "Epoch: 71, Accuracy: 0.9374\n",
      "Epoch: 72, Accuracy: 0.9368\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / sum(e_z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "class Network2(object):\n",
    "    def __init__(self, sizes, l2_lambda=1e-5, momentum=0.5):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.momentum = momentum\n",
    "        self.prev_dLdWs = [np.zeros_like(w) for w in self.weights]\n",
    "        self.prev_dLdBs = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        a = a.T\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        a = softmax(np.dot(self.weights[-1], a) + self.biases[-1])\n",
    "        return a\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b, nabla_w = self.backprop(mini_batch[0].T, mini_batch[1].T)\n",
    "\n",
    "        # Update weights and biases with L2 regularization and momentum\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = (1 -  self.l2_lambda) * self.weights[i] - eta / len(mini_batch[0]) * nabla_w[i] + self.momentum * self.prev_dLdWs[i]\n",
    "            self.biases[i] = self.biases[i] - eta / len(mini_batch[0]) * nabla_b[i] + self.momentum * self.prev_dLdBs[i]\n",
    "\n",
    "            # Update previous gradients for momentum\n",
    "            self.prev_dLdWs[i] = -eta / len(mini_batch[0]) * nabla_w[i] + self.momentum * self.prev_dLdWs[i]\n",
    "            self.prev_dLdBs[i] = -eta / len(mini_batch[0]) * nabla_b[i] + self.momentum * self.prev_dLdBs[i]\n",
    "\n",
    "\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a pair of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "        g = x\n",
    "        gs = [g] # list to store all the gs, layer by layer\n",
    "        fs = [] # list to store all the fs, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            f = np.dot(w, g) + b\n",
    "            fs.append(f)\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "\n",
    "        # Last layer using softmax\n",
    "        f = np.dot(self.weights[-1], g) + self.biases[-1]\n",
    "        fs.append(f)\n",
    "        g = softmax(f)\n",
    "        gs.append(g)\n",
    "        dLdg = self.cost_derivative(gs[-1], y)\n",
    "        dLdfs = []\n",
    "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
    "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
    "            dLdfs.append(dLdf)\n",
    "            dLdg = np.matmul(w.T, dLdf)\n",
    "\n",
    "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])]\n",
    "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)]\n",
    "        return (dLdBs,dLdWs)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
    "        corr = np.argmax(test_data[1],axis=1).T\n",
    "        return np.mean(pred==corr)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        x_train, y_train = training_data\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "        for j in range(epochs):\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
    "            else:\n",
    "                print(\"Epoch: {0}\".format(j))\n",
    "\n",
    "\n",
    "network2 = Network2([784,30,10])\n",
    "network2.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTFvgo23lgPh"
   },
   "source": [
    "**Task 3 (optional).** Implement Adagrad, dropout and some simple data augmentations (e.g. tiny rotations/shifts etc.). Again, test to see how these changes improve accuracy/convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh4XXZZQd2Cy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE5h9Kk9lhsQ"
   },
   "source": [
    "\n",
    "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "BKBj9u_cX6-O",
    "outputId": "99485c32-949b-42f1-ed8d-db5e6d553bf6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e9e1231c419>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mnetwork2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mnetwork2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3e9e1231c419>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sizes, l2_lambda, momentum)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-3e9e1231c419>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return e_z / sum(e_z)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    # Derivative of the sigmoid\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "\n",
    "class Network2(object):\n",
    "    def __init__(self, sizes, l2_lambda=1e-4, momentum=0.4):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.momentum = momentum\n",
    "        self.prev_dLdWs = [np.zeros_like(w) for w in self.weights]\n",
    "        self.prev_dLdBs = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        a = a.T\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        a = softmax(np.dot(self.weights[-1], a) + self.biases[-1])\n",
    "        return a\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b, nabla_w = self.backprop(mini_batch[0].T, mini_batch[1].T)\n",
    "\n",
    "        # Update weights and biases with L2 regularization and momentum\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = (1 -  self.l2_lambda) * self.weights[i] - eta / len(mini_batch[0]) * nabla_w[i] + self.momentum * self.prev_dLdWs[i]\n",
    "            self.biases[i] = self.biases[i] - eta / len(mini_batch[0]) * nabla_b[i] + self.momentum * self.prev_dLdBs[i]\n",
    "\n",
    "            # Update previous gradients for momentum\n",
    "            self.prev_dLdWs[i] = -eta / len(mini_batch[0]) * nabla_w[i] + self.momentum * self.prev_dLdWs[i]\n",
    "            self.prev_dLdBs[i] = -eta / len(mini_batch[0]) * nabla_b[i] + self.momentum * self.prev_dLdBs[i]\n",
    "\n",
    "\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        # For a single input (x,y) return a pair of lists.\n",
    "        # First contains gradients over biases, second over weights.\n",
    "        g = x\n",
    "        gs = [g] # list to store all the gs, layer by layer\n",
    "        fs = [] # list to store all the fs, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            f = np.dot(w, g) + b\n",
    "            fs.append(f)\n",
    "            g = sigmoid(f)\n",
    "            gs.append(g)\n",
    "\n",
    "        # Last layer using softmax\n",
    "        f = np.dot(self.weights[-1], g) + self.biases[-1]\n",
    "        fs.append(f)\n",
    "        g = softmax(f)\n",
    "        gs.append(g)\n",
    "        dLdg = self.cost_derivative(gs[-1], y)\n",
    "        dLdfs = []\n",
    "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
    "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
    "            dLdfs.append(dLdf)\n",
    "            dLdg = np.matmul(w.T, dLdf)\n",
    "\n",
    "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])]\n",
    "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)]\n",
    "        return (dLdBs,dLdWs)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        # Count the number of correct answers for test_data\n",
    "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
    "        corr = np.argmax(test_data[1],axis=1).T\n",
    "        return np.mean(pred==corr)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        x_train, y_train = training_data\n",
    "        if test_data:\n",
    "            x_test, y_test = test_data\n",
    "        for j in range(epochs):\n",
    "            for i in range(x_train.shape[0] // mini_batch_size):\n",
    "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
    "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
    "            else:\n",
    "                print(\"Epoch: {0}\".format(j))\n",
    "\n",
    "\n",
    "network2 = Network2([784,100,30,10])\n",
    "network2.SGD((x_train, y_train), epochs=100, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
